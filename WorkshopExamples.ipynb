{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>UofT Social Science Methods Week</h3>\n",
    "<h1>Introduction to Machine Learning for Textual Analysis</h1>\n",
    "\n",
    "<p>\n",
    "This notebook illustrates the key steps to fit a machine learning classifier using textual data.  We will use the sklearn library for Python and focus mostly on support vector machines (SVMs).\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We start by importing libraries for the current session.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These classes provide \"vectorizers\" to encode texts into a term-document matrix: \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# This class provides a support vector classifier with a linear kernel:\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# This class will be useful later on to assess the model using cross-validation:\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# This class provides accuracy metrics.\n",
    "from sklearn import metrics\n",
    "\n",
    "# These libraries are useful to deal with maths and data frames.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Toy Example</h2>\n",
    "\n",
    "Let us start by looking at a simple example to make sure that we understand each step involved in the estimation and interpretation of SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Four text documents stored in a Python list.\n",
    "documents = [\"Chinese Beijing Chinese\", \n",
    "            \"Chinese Chinese Shanghai\",\n",
    "            \"Chinese Macao\",\n",
    "            \"Tokyo Japan Chinese\"]\n",
    "\n",
    "# The corresponding classes, annotated.\n",
    "y = [\"China\", \"China\", \"China\", \"Not China\"]\n",
    "\n",
    "# A new document for which we want to predict the class.\n",
    "new_document = [\"Chinese Chinese Japan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Transforming the texts into a term-document matrix</h3>\n",
    "\n",
    "We create an instance of the CountVectorizer class, and call it, for instance, \"vectorizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform the text documents into a term-document matrix, with the fit_transform function of the CountVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the result object X, we can see that it is stored as a sparse matrix to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert it to a dense matrix to visualize the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary_ attribute is a dictionary that maps each word in the vocabulary to the columns in the matrix X.  Notice that Python is zero-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_feature_names() function returns an ordered list of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Fitting the model</h3>\n",
    "\n",
    "We can now create an instance of the LinearSVC class, here simply named \"model\".  This will be our classifier.  Without arguments, the model will rely on default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model with the fit function, and enter X (the design matrix/TDM) and y as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the coefficients of the hyperplane with the coef_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an intercept, which we can also retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clearer, let's look at the hyperplane estimates for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, beta in enumerate(model.coef_[0]):\n",
    "    # Iterate through the coefficients.\n",
    "    print(\"%s: %0.3f\" %(vectorizer.get_feature_names()[i], beta))\n",
    "    # Print the name of the column and the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand how the classes have been encoded (as -1 and 1).  They were stored in alpha order by default, such that the positive class is \"Not China\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify on which side of the hyperplane each of the four documents falls, by computing $\\hat{\\alpha} + \\mathbf{x}\\hat{\\boldsymbol\\beta}$. Documents with negative values, below the hyperplane, are in the class \"China\", whereas the document above is in the class \"Not China\".  This is what we expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(X.todense()):\n",
    "    # Iterate the rows of the X matrix.\n",
    "    yhat = model.intercept_[0] + np.dot(row, np.transpose(model.coef_[0]))[0,0]\n",
    "    # For each row, compute alpha + the dot product of x and the coefficients.\n",
    "    print(\"%s: %0.3f\" %(documents[i], yhat))\n",
    "    # Print the document with the resulting \"yhat\" value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Predicting the class of new documents</h3>\n",
    "\n",
    "Now that the model is trained, we can use it to predict the class of a new document.  We have set aside one such example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the document into the same vector space.  We invoke the vectorizer, this time with the transform method.  This implies that words that have never been observed when fitting the model cannot be used to predict new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xprime = vectorizer.transform(new_document)\n",
    "Xprime.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the class is simple.  We invoke our model with the predict function and pass as an argument the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted the category \"China\", which sounds good.  The example was ambiguous as it contained a word from the other category.  We can confirm where it falls relative to the hyperplane, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_[0] + np.dot(Xprime.todense()[0], np.transpose(model.coef_[0]))[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Example 2. Sentiment Analysis with Social Media Data</h2>\n",
    "\n",
    "Here's a full example using a real-world dataset of social media data.  This is a sample from the Stanford Twitter dataset, with tweets annotated for sentiment.  The example relies on pandas for Python to handle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"socialmedia.csv\", sep=',', encoding=\"utf-8\", header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 10,000 annotated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awareness of the class distribution is primordial.  Here, we have a near-balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Transforming the texts into a term-document matrix</h3>\n",
    "\n",
    "Let us create Python lists containing the documents and the sentiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df.text.tolist()\n",
    "y = df.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create our vectorizer.  This example relies on a TF-IDF weighted term-document matrix restricted to the 5000 most frequent terms, and we remove English stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the text into a vector space, as we did before.  We can confirm that the matrix has shape 10000 x 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(text)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Fitting and assessing the model using cross-validation</h3>\n",
    "\n",
    "The simplest way to perform cross-validation is to split the sample at random between training and testing set.  This can be done easily with the train_test_split() function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the accuracy and F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, y_predicted, pos_label='Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's write a function that retrieves the largest coefficients and indicates which words are best predictors of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showTopFeatures(classifier, vectorizer, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    temp_pos = np.argsort(classifier.coef_)[:,-n:].tolist()[0]\n",
    "    temp_neg = np.argsort(classifier.coef_)[:,0:n].tolist()[0]\n",
    "    print(\"Top positive features:\\n\")\n",
    "    for t in temp_pos: \n",
    "        print(\"%s\" % (feature_names[t]))\n",
    "    print(\"\\n\")\n",
    "    print(\"Top negative features:\\n\")    \n",
    "    for t in temp_neg: \n",
    "        print(\"%s\" % (feature_names[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTopFeatures(model, vectorizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Improving the model with hyperparameter optimization and feature selection</h3>\n",
    "\n",
    "We have left the hyperparameter C at the default value, 1.  We may adjust the parameter to find a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example of hyperparameter optimization.\n",
    "for c in [0.01,0.05,0.1,1,5,10]:\n",
    "    model = LinearSVC(C=c)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    print(\"F1-Score for C = %s: %0.3f\" %(str(c), metrics.f1_score(y_test, y_predicted, pos_label='Positive')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting C = 0.05 increases the accuracy of prediction.\n",
    "\n",
    "Next, we can compare the performance of various classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [('Support Vector Machine', LinearSVC(C=.05)),\n",
    "               ('Decision Tree', DecisionTreeClassifier()), \n",
    "               ('Logistic Regression', LogisticRegression(penalty='l1'))]\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    model = clf\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    print(\"Classifier: %s\" %name)\n",
    "    print(\"Accuracy: %0.3f\" %metrics.accuracy_score(y_test, y_predicted))\n",
    "    print(\"F1-Score: %0.3f\" %metrics.f1_score(y_test, y_predicted, pos_label='Positive'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection helps to achieve a better bias-variance trade-off, by removing poor predictors from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=10000)\n",
    "kbest = SelectKBest(chi2, k=2000)\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "X = kbest.fit_transform(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "print(\"Accuracy: %0.3f\" %metrics.accuracy_score(y_test, y_predicted))\n",
    "print(\"F1-Score: %0.3f\" %metrics.f1_score(y_test, y_predicted, pos_label='Positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Making predictions</h3>\n",
    "\n",
    "If we are satisfied with the model, we can fit the chosen specification on the full sample and save it for later use.  In this case, we will use it to predict the sentiment of new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=5)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load a new dataset with tweets, with the aim of predicting sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.read_table(\"unseen_documents.csv\", sep=',', encoding=\"utf-8\", header=0)\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the new documents into the same vector space.  This now requires two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newtext = newdf.text.tolist()\n",
    "newX = vectorizer.transform(newtext)\n",
    "newX = kbest.transform(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new variable in our dataset with the predictions from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf['sentiment'] = model.predict(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
